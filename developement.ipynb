{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, re, time\n",
    "from datetime import datetime\n",
    "import ujson as json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_page(url):\n",
    "    '''Get the page content\n",
    "\n",
    "    Args:\n",
    "        url (str): url to scrape\n",
    "\n",
    "    Returns:\n",
    "        response (obj): object of web content\n",
    "    '''\n",
    "    time.sleep(3)\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:73.0) Gecko/20100101 Firefox/73.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response\n",
    "    else:\n",
    "        print(f'Something went wrong when scraping {url}')\n",
    "\n",
    "def copy_page(url):\n",
    "    '''Copy the web content into local disk\n",
    "\n",
    "    Args:\n",
    "        url (str): url of web page\n",
    "    '''\n",
    "    response = get_page(url)\n",
    "    with open('check.html', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "        \n",
    "def save_data(news_title, news_contents):\n",
    "    '''Save news_title and news_contents in data file\n",
    "    \n",
    "    Parameters\n",
    "        news_title (str): news title\n",
    "        news_content (str): news content\n",
    "    '''\n",
    "    file_name = 'train.csv'\n",
    "    file = os.path.join('data', file_name)\n",
    "    if os.path.exists(file): \n",
    "        mode = 'a'\n",
    "    else:\n",
    "        mode = 'w'\n",
    "    # write in the data\n",
    "    with open(file, mode=mode, newline='') as csv_file:\n",
    "        fieldnames = ['Title', 'Contents']\n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        if mode == 'w': csv_writer.writeheader()\n",
    "        data_dict = dict(zip(fieldnames, [news_title, news_contents]))\n",
    "        csv_writer.writerow(data_dict)\n",
    "\n",
    "    print(f'{news_title} written in successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeLiberty():\n",
    "    \"\"\"Download Liberty times news\n",
    "    \n",
    "    Functions:\n",
    "        scrape: scrape the news in categories\n",
    "    \"\"\"\n",
    "    def __init__(self, data=None):\n",
    "        self.url_base = 'https://news.ltn.com.tw/ajax/breakingnews'\n",
    "        self.categories = {'entertainment': 'entertainment',\n",
    "                           'politics': 'politic',\n",
    "                           'sports': 'sport',\n",
    "                           'society': 'society'}\n",
    "        self.data = data\n",
    "        self.titles = self.data[:, 2] if self.data else []\n",
    "    \n",
    "    def scrape(self):\n",
    "        data = []\n",
    "        for category in self.categories.keys():\n",
    "            index = 1\n",
    "            \n",
    "            while True:\n",
    "                url = f'{self.url_base}/{category}/{index}'\n",
    "                news_list = get_page(url)\n",
    "                news_list = json.loads(news_list.text)['data']\n",
    "                if news_list == []: break\n",
    "                \n",
    "                with tqdm(total=len(news_list)) as pbar:\n",
    "                    for news in news_list:\n",
    "                        try:\n",
    "                            datum = self.process_page(news, category)\n",
    "                        except: datum = None\n",
    "                        if datum:\n",
    "                            data.append(datum)\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix(index=index, category=self.categories[category])\n",
    "                index += 1\n",
    "        \n",
    "        if self.data is None:\n",
    "            self.data = np.array(data)\n",
    "        else:\n",
    "            self.data = np.concatenate([np.array(data), self.data], dim=0)\n",
    "    \n",
    "    def process_page(self, news, category):\n",
    "        url = news['url']\n",
    "        response = get_page(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        article = self.get_article(soup, category)\n",
    "\n",
    "        title = self.get_title(article)\n",
    "        if title in self.titles:\n",
    "            return None\n",
    "\n",
    "        date = self.get_date(article, category)\n",
    "        content = self.get_content(article, category)\n",
    "        image_path = self.get_image(news['photo_S'])\n",
    "\n",
    "        return [date, title, content, image_path, self.categories[category], url]\n",
    "    \n",
    "    def get_article(self, soup, category):\n",
    "        if category in ['entertainment', 'sports']:\n",
    "            return soup.find('div', class_=\"content\")\n",
    "        else:\n",
    "            return soup.find(itemprop=\"articleBody\")\n",
    "\n",
    "    def get_date(self, article, category):\n",
    "        if category in ['entertainment', 'sports']:\n",
    "            pattern = '\\d{4}\\/\\d{2}\\/\\d{2}'\n",
    "            date = re.search(pattern, article.text).group()\n",
    "            date = datetime.strptime(date, '%Y/%m/%d').date()\n",
    "        else:\n",
    "            pattern = '\\d{4}-\\d{2}-\\d{2}'\n",
    "            date = re.search(pattern, article.text).group()\n",
    "            date = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        return date\n",
    "\n",
    "    def get_title(self, article):\n",
    "        return article.h1.text\n",
    "\n",
    "    def get_content(self, article, category):\n",
    "        if category in ['entertainment']:\n",
    "            news_content_list = article.find_all('p', class_='')\n",
    "            content = ''.join([content.text for content in news_content_list if not content.span])\n",
    "        elif category in ['sports']:\n",
    "            news_content_list = article.find_all('p', class_='')\n",
    "            content = ''.join([content.text for content in news_content_list if not content.img])\n",
    "        else:\n",
    "            news_content_list = article.find('div', class_='text boxTitle boxText').find_all('p', class_='', recursive=False)\n",
    "            content = ''.join([content.text for content in news_content_list])\n",
    "        return content\n",
    "    \n",
    "    def get_image(self, url):\n",
    "        image_name = url.split('/')[-1]\n",
    "        image_path = os.path.join('data', 'pictures', 'liberty', image_name)\n",
    "        image = get_page(url)\n",
    "        with open(os.path.join(os.getcwd(), image_path), 'wb') as f:\n",
    "            for chunk in image:\n",
    "                f.write(chunk)\n",
    "        return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeChina():\n",
    "    \"\"\"Download China times news\n",
    "    \n",
    "    Functions:\n",
    "        scrape: scrape the news in categories\n",
    "    \"\"\"\n",
    "    def __init__(self, data=None):\n",
    "        self.url_base = 'https://www.chinatimes.com'\n",
    "        self.categories = {'star': 'entertainment',\n",
    "                           'politic': 'politic',\n",
    "                           'sports': 'sport',\n",
    "                           'society': 'society'}\n",
    "        self.data = data\n",
    "        self.titles = self.data[:, 2] if self.data else []\n",
    "    \n",
    "    def scrape(self):\n",
    "        data = []\n",
    "        for category in self.categories.keys():\n",
    "            for index in range(1, 11):  \n",
    "                url = f'{self.url_base}/{category}/total?page={index}&chdtv'\n",
    "                response = get_page(url)\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "                news_list = soup.find('div', class_=\"container\") \\\n",
    "                                .find('ul', class_='vertical-list') \\\n",
    "                                .find_all('li')\n",
    "                \n",
    "                with tqdm(total=len(news_list)) as pbar:\n",
    "                    for news in news_list:\n",
    "                        try:\n",
    "                            datum = self.process_page(news, category)\n",
    "                        except: datum = None\n",
    "                        if datum:\n",
    "                            data.append(datum)\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix(index=index, category=self.categories[category])\n",
    "        \n",
    "        if self.data is None:\n",
    "            self.data = np.array(data)\n",
    "        else:\n",
    "            self.data = np.concatenate([np.array(data), self.data], dim=0)\n",
    "    \n",
    "    def process_page(self, news, category):\n",
    "        url = f'{self.url_base}{news.h3.a[\"href\"]}'\n",
    "        response = get_page(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        article = self.get_article(soup, category)\n",
    "\n",
    "        title = self.get_title(article)\n",
    "        if title in self.titles:\n",
    "            return None\n",
    "\n",
    "        date = self.get_date(article, category)\n",
    "        content = self.get_content(article, category)\n",
    "        image_path = self.get_image(news.img[\"src\"])\n",
    "\n",
    "        return [date, title, content, image_path, self.categories[category], url]\n",
    "    \n",
    "    def get_article(self, soup, category):\n",
    "        return soup.article\n",
    "\n",
    "    def get_date(self, article, category):\n",
    "        pattern = '\\d{4}\\/\\d{2}\\/\\d{2}'\n",
    "        date = re.search(pattern, article.time.text).group()\n",
    "        date = datetime.strptime(date, '%Y/%m/%d').date()\n",
    "        return date\n",
    "\n",
    "    def get_title(self, article):\n",
    "        return article.h1.text\n",
    "\n",
    "    def get_content(self, article, category):\n",
    "        news_content_list = article.find('div', class_='article-body').find_all('p')\n",
    "        content = ''.join([content.text for content in news_content_list])\n",
    "        return content\n",
    "    \n",
    "    def get_image(self, url):\n",
    "        image_name = url.split('/')[-1]\n",
    "        image_path = os.path.join('data', 'pictures', 'china', image_name)\n",
    "        image = get_page(url)\n",
    "        with open(os.path.join(os.getcwd(), image_path), 'wb') as f:\n",
    "            for chunk in image:\n",
    "                f.write(chunk)\n",
    "        return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:08<00:00,  6.42s/it, category=entertainment, index=1]\n",
      "100%|██████████| 20/20 [02:08<00:00,  6.42s/it, category=politic, index=1]\n",
      "100%|██████████| 20/20 [02:04<00:00,  6.24s/it, category=sport, index=1]\n",
      "100%|██████████| 20/20 [02:07<00:00,  6.38s/it, category=society, index=1]\n"
     ]
    }
   ],
   "source": [
    "class ScrapeUDN():\n",
    "    \"\"\"Download UDN times news\n",
    "    \n",
    "    Functions:\n",
    "        scrape: scrape the news in categories\n",
    "    \"\"\"\n",
    "    def __init__(self, data=None):\n",
    "        self.url_base = 'https://www.chinatimes.com'\n",
    "        self.categories = {'star': 'entertainment',\n",
    "                           'politic': 'politic',\n",
    "                           'sports': 'sport',\n",
    "                           'society': 'society'}\n",
    "        self.data = data\n",
    "        self.titles = self.data[:, 2] if self.data else []\n",
    "    \n",
    "    def scrape(self):\n",
    "        data = []\n",
    "        for category in self.categories.keys():\n",
    "            for index in range(1, 11):  \n",
    "                url = f'{self.url_base}/{category}/total?page={index}&chdtv'\n",
    "                response = get_page(url)\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "                news_list = soup.find('div', class_=\"container\") \\\n",
    "                                .find('ul', class_='vertical-list') \\\n",
    "                                .find_all('li')\n",
    "                \n",
    "                with tqdm(total=len(news_list)) as pbar:\n",
    "                    for news in news_list:\n",
    "                        try:\n",
    "                            datum = self.process_page(news, category)\n",
    "                        except: datum = None\n",
    "                        if datum:\n",
    "                            data.append(datum)\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix(index=index, category=self.categories[category])\n",
    "        \n",
    "        if self.data is None:\n",
    "            self.data = np.array(data)\n",
    "        else:\n",
    "            self.data = np.concatenate([np.array(data), self.data], dim=0)\n",
    "    \n",
    "    def process_page(self, news, category):\n",
    "        url = f'{self.url_base}{news.h3.a[\"href\"]}'\n",
    "        response = get_page(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        article = self.get_article(soup, category)\n",
    "\n",
    "        title = self.get_title(article)\n",
    "        if title in self.titles:\n",
    "            return None\n",
    "\n",
    "        date = self.get_date(article, category)\n",
    "        content = self.get_content(article, category)\n",
    "        image_path = self.get_image(news.img[\"src\"])\n",
    "\n",
    "        return [date, title, content, image_path, self.categories[category], url]\n",
    "    \n",
    "    def get_article(self, soup, category):\n",
    "        return soup.article\n",
    "\n",
    "    def get_date(self, article, category):\n",
    "        pattern = '\\d{4}\\/\\d{2}\\/\\d{2}'\n",
    "        date = re.search(pattern, article.time.text).group()\n",
    "        date = datetime.strptime(date, '%Y/%m/%d').date()\n",
    "        return date\n",
    "\n",
    "    def get_title(self, article):\n",
    "        return article.h1.text\n",
    "\n",
    "    def get_content(self, article, category):\n",
    "        news_content_list = article.find('div', class_='article-body').find_all('p')\n",
    "        content = ''.join([content.text for content in news_content_list])\n",
    "        return content\n",
    "    \n",
    "    def get_image(self, url):\n",
    "        image_name = url.split('/')[-1]\n",
    "        image_path = os.path.join('data', 'pictures', 'china', image_name)\n",
    "        image = get_page(url)\n",
    "        with open(os.path.join(os.getcwd(), image_path), 'wb') as f:\n",
    "            for chunk in image:\n",
    "                f.write(chunk)\n",
    "        return image_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
