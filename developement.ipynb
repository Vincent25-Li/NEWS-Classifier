{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, re, time\n",
    "from datetime import datetime\n",
    "import ujson as json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_page(url):\n",
    "    '''Get the page content\n",
    "\n",
    "    Args:\n",
    "        url (str): url to scrape\n",
    "\n",
    "    Returns:\n",
    "        response (obj): object of web content\n",
    "    '''\n",
    "    time.sleep(6)\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:73.0) Gecko/20100101 Firefox/73.0'}\n",
    "    response = requests.get(url, headers=headers, timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        return response\n",
    "    else:\n",
    "        print(f'Something went wrong when scraping {url}')\n",
    "\n",
    "def copy_page(url):\n",
    "    '''Copy the web content into local disk\n",
    "\n",
    "    Args:\n",
    "        url (str): url of web page\n",
    "    '''\n",
    "    response = get_page(url)\n",
    "    with open('check.html', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "        \n",
    "def save_data(news_title, news_contents):\n",
    "    '''Save news_title and news_contents in data file\n",
    "    \n",
    "    Parameters\n",
    "        news_title (str): news title\n",
    "        news_content (str): news content\n",
    "    '''\n",
    "    file_name = 'train.csv'\n",
    "    file = os.path.join('data', file_name)\n",
    "    if os.path.exists(file): \n",
    "        mode = 'a'\n",
    "    else:\n",
    "        mode = 'w'\n",
    "    # write in the data\n",
    "    with open(file, mode=mode, newline='') as csv_file:\n",
    "        fieldnames = ['Title', 'Contents']\n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        if mode == 'w': csv_writer.writeheader()\n",
    "        data_dict = dict(zip(fieldnames, [news_title, news_contents]))\n",
    "        csv_writer.writerow(data_dict)\n",
    "\n",
    "    print(f'{news_title} written in successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeLiberty():\n",
    "    \"\"\"Download Liberty times news\n",
    "    \n",
    "    Functions:\n",
    "        scrape: scrape the news in categories\n",
    "    \"\"\"\n",
    "    def __init__(self, data=None):\n",
    "        self.url_base = 'https://news.ltn.com.tw/ajax/breakingnews'\n",
    "        self.categories = {'entertainment': 'entertainment',\n",
    "                           'politics': 'politic',\n",
    "                           'sports': 'sport',\n",
    "                           'society': 'society'}\n",
    "        self.news = 'liberty'\n",
    "        self.data = data\n",
    "        self.titles = self.data[:, 2] if self.data else []\n",
    "    \n",
    "    def scrape(self):\n",
    "        data = []\n",
    "        for category in self.categories.keys():\n",
    "            index = 1\n",
    "            \n",
    "            while True:\n",
    "                url = f'{self.url_base}/{category}/{index}'\n",
    "                news_list = get_page(url)\n",
    "                news_list = json.loads(news_list.text)['data']\n",
    "                if news_list == []: break\n",
    "                \n",
    "                with tqdm(total=len(news_list)) as pbar:\n",
    "                    for news in news_list:\n",
    "                        try:\n",
    "                            datum = self.process_page(news, category)\n",
    "                        except: datum = None\n",
    "                        if datum:\n",
    "                            data.append(datum)\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix(index=index, category=self.categories[category], news=self.news)\n",
    "                index += 1\n",
    "        \n",
    "        if self.data is None:\n",
    "            self.data = np.array(data)\n",
    "        else:\n",
    "            self.data = np.concatenate([np.array(data), self.data], dim=0)\n",
    "    \n",
    "    def process_page(self, news, category):\n",
    "        url = news['url']\n",
    "        response = get_page(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        article = self.get_article(soup, category)\n",
    "\n",
    "        title = self.get_title(article)\n",
    "        if title in self.titles:\n",
    "            return None\n",
    "\n",
    "        date = self.get_date(article, category)\n",
    "        content = self.get_content(article, category)\n",
    "        image_path = self.get_image(news['photo_S'])\n",
    "\n",
    "        return [date, title, content, image_path, self.categories[category], url]\n",
    "    \n",
    "    def get_article(self, soup, category):\n",
    "        if category in ['entertainment', 'sports']:\n",
    "            return soup.find('div', class_=\"content\")\n",
    "        else:\n",
    "            return soup.find(itemprop=\"articleBody\")\n",
    "\n",
    "    def get_date(self, article, category):\n",
    "        if category in ['entertainment', 'sports']:\n",
    "            pattern = '\\d{4}\\/\\d{2}\\/\\d{2}'\n",
    "            date = re.search(pattern, article.text).group()\n",
    "            date = datetime.strptime(date, '%Y/%m/%d').date()\n",
    "        else:\n",
    "            pattern = '\\d{4}-\\d{2}-\\d{2}'\n",
    "            date = re.search(pattern, article.text).group()\n",
    "            date = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        return date\n",
    "\n",
    "    def get_title(self, article):\n",
    "        return article.h1.text\n",
    "\n",
    "    def get_content(self, article, category):\n",
    "        if category in ['entertainment']:\n",
    "            news_content_list = article.find_all('p', class_='')\n",
    "            content = ''.join([content.text for content in news_content_list if not content.span])\n",
    "        elif category in ['sports']:\n",
    "            news_content_list = article.find_all('p', class_='')\n",
    "            content = ''.join([content.text for content in news_content_list if not content.img])\n",
    "        else:\n",
    "            news_content_list = article.find('div', class_='text boxTitle boxText').find_all('p', class_='', recursive=False)\n",
    "            content = ''.join([content.text for content in news_content_list])\n",
    "        return content\n",
    "    \n",
    "    def get_image(self, url):\n",
    "        image_name = url.split('/')[-1]\n",
    "        image_path = os.path.join('data', 'pictures', self.news, image_name)\n",
    "        image = get_page(url)\n",
    "        with open(os.path.join(os.getcwd(), image_path), 'wb') as f:\n",
    "            for chunk in image:\n",
    "                f.write(chunk)\n",
    "        return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeChina(ScrapeLiberty):\n",
    "    \"\"\"Download China times news\n",
    "    \n",
    "    Functions:\n",
    "        scrape: scrape the news in categories\n",
    "    \"\"\"\n",
    "    def __init__(self, data=None):\n",
    "        ScrapeLiberty.__init__(self, data)\n",
    "        self.url_base = 'https://www.chinatimes.com'\n",
    "        self.categories = {'star': 'entertainment',\n",
    "                           'politic': 'politic',\n",
    "                           'sports': 'sport',\n",
    "                           'society': 'society'}\n",
    "        self.news = 'china'\n",
    "    \n",
    "    def scrape(self):\n",
    "        data = []\n",
    "        for category in self.categories.keys():\n",
    "            for index in range(1, 11):  \n",
    "                url = f'{self.url_base}/{category}/total?page={index}&chdtv'\n",
    "                response = get_page(url)\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "                news_list = soup.find('div', class_=\"container\") \\\n",
    "                                .find('ul', class_='vertical-list') \\\n",
    "                                .find_all('li')\n",
    "                \n",
    "                with tqdm(total=len(news_list)) as pbar:\n",
    "                    for news in news_list:\n",
    "                        try:\n",
    "                            datum = self.process_page(news, category)\n",
    "                        except: datum = None\n",
    "                        if datum:\n",
    "                            data.append(datum)\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix(index=index, category=self.categories[category], news=self.news)\n",
    "        \n",
    "        if self.data is None:\n",
    "            self.data = np.array(data)\n",
    "        else:\n",
    "            self.data = np.concatenate([np.array(data), self.data], dim=0)\n",
    "    \n",
    "    def process_page(self, news, category):\n",
    "        url = f'{self.url_base}{news.h3.a[\"href\"]}'\n",
    "        response = get_page(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        article = self.get_article(soup)\n",
    "\n",
    "        title = self.get_title(article)\n",
    "        if title in self.titles:\n",
    "            return None\n",
    "\n",
    "        date = self.get_date(article, category)\n",
    "        content = self.get_content(article, category)\n",
    "        image_path = self.get_image(news.img[\"src\"])\n",
    "\n",
    "        return [date, title, content, image_path, self.categories[category], url]\n",
    "    \n",
    "    def get_article(self, soup):\n",
    "        return soup.article\n",
    "    \n",
    "    def get_date(self, article, category):\n",
    "        pattern = '\\d{4}\\/\\d{2}\\/\\d{2}'\n",
    "        date = re.search(pattern, article.time.text).group()\n",
    "        date = datetime.strptime(date, '%Y/%m/%d').date()\n",
    "        return date\n",
    "\n",
    "    def get_content(self, article, category):\n",
    "        news_content_list = article.find('div', class_='article-body').find_all('p')\n",
    "        content = ''.join([content.text for content in news_content_list])\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeUDN(ScrapeLiberty):\n",
    "    \"\"\"Download UDN news\n",
    "    \n",
    "    Functions:\n",
    "        scrape: scrape the news in categories\n",
    "    \"\"\"\n",
    "    def __init__(self, data=None):\n",
    "        ScrapeLiberty.__init__(self, data)\n",
    "        self.url_base = 'https://udn.com'\n",
    "        self.url_base_s = 'https://stars.udn.com'\n",
    "        self.categories = {'stock': (6645, 90),\n",
    "                           'sport': (7227, 193),\n",
    "                           'society': (6639, 123),\n",
    "                           'entertainment': 'stars'}\n",
    "        self.news = 'udn'\n",
    "    \n",
    "    def scrape(self):\n",
    "        data = []\n",
    "        for category in self.categories.keys():\n",
    "            index = 0\n",
    "            counts = 0\n",
    "            counts_e = 0\n",
    "            while True: \n",
    "                url = self.get_category_url(category, index)\n",
    "                response = get_page(url)\n",
    "                news_list = self.get_news_list(response, category)\n",
    "                if not news_list: break\n",
    "                \n",
    "                with tqdm(total=len(news_list)) as pbar:\n",
    "                    for news in news_list:\n",
    "                        try:\n",
    "                            datum = self.process_page(news, category)\n",
    "                        except: datum = None\n",
    "                        if datum:\n",
    "                            data.append(datum)\n",
    "                            if category == 'entertainment': \n",
    "                                counts_e += 1\n",
    "                            else:\n",
    "                                counts += 1\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix(index=index, category=category, news=self.news)\n",
    "                if counts_e > (counts / 3): break\n",
    "                index += 1\n",
    "                break\n",
    "        \n",
    "        if self.data is None:\n",
    "            self.data = np.array(data)\n",
    "        else:\n",
    "            self.data = np.concatenate([np.array(data), self.data], dim=0)\n",
    "    \n",
    "    def process_page(self, news, category):\n",
    "        url = self.get_article_url(news, category)\n",
    "        response = get_page(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        article = self.get_article(soup, category)\n",
    "\n",
    "        title = self.get_title(article)\n",
    "        if title in self.titles:\n",
    "            return None\n",
    "\n",
    "        date = self.get_date(article)\n",
    "        content = self.get_content(article, category)\n",
    "        img_url =  news.find('a', class_='item-image').img['data-original'] \\\n",
    "                    if category == 'entertainment' else news['url']\n",
    "        image_path = self.get_image(img_url)\n",
    "\n",
    "        return [date, title, content, image_path, category, url]\n",
    "    \n",
    "    def get_category_url(self, category, index):\n",
    "        if category == 'entertainment':\n",
    "            url = f'{self.url_base_s}/common/ajax_show_more/news/{index + 1}/0/0/0'\n",
    "        else:\n",
    "            (cate_id, totalRecNo) = self.categories[category]\n",
    "            url_remaining = f'/api/more?page={index}&channelId=2&type=cate_latest_news&cate_id={cate_id}&totalRecNo={totalRecNo}'\n",
    "            url = self.url_base + url_remaining\n",
    "        return url\n",
    "    \n",
    "    def get_news_list(self, response, category):\n",
    "        if category == 'entertainment':\n",
    "            response = json.loads(response.text)\n",
    "            soup = BeautifulSoup(response['_html'], 'lxml')\n",
    "            news_list = soup.find_all('div', class_='item')\n",
    "        else:\n",
    "            news_list = json.loads(response.text)\n",
    "            news_list = news_list['lists'] if 'lists' in news_list else None\n",
    "        return news_list\n",
    "    \n",
    "    def get_article_url(self, news, category):\n",
    "        if category == 'entertainment':\n",
    "            url = f'{self.url_base_s}{news.find(\"div\", class_=\"item-text\").a[\"href\"]}'\n",
    "        else:\n",
    "            url = f'{self.url_base}{news[\"titleLink\"]}'\n",
    "        return url\n",
    "    \n",
    "    def get_article(self, soup, category):\n",
    "        if category == 'entertainment':\n",
    "            article = soup.find('section', class_='cate-article')\n",
    "        else:\n",
    "            article = soup.find('section', class_='article-content__wrapper')\n",
    "        return article\n",
    "    \n",
    "    def get_date(self, article):\n",
    "        pattern = '\\d{4}-\\d{2}-\\d{2}'\n",
    "        date = re.search(pattern, article.text).group()\n",
    "        date = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        return date\n",
    "\n",
    "    def get_content(self, article, category):\n",
    "        if category == 'entertainment':\n",
    "            news_content_list = article.find('div', class_='article').find_all('p')\n",
    "        else:\n",
    "            news_content_list = article.find('div', class_='article-content__paragraph').find_all('p')\n",
    "            \n",
    "        content = ''.join([content.text for content in news_content_list])\n",
    "        content = content.replace('\\r', '').replace('\\n', '')\n",
    "        return content\n",
    "    \n",
    "    def get_image(self, url):\n",
    "        image_name = url.split('/')[-1].split('&')[0]\n",
    "        image_path = os.path.join('data', 'pictures', self.news, image_name)\n",
    "        image = get_page(url)\n",
    "        with open(os.path.join(os.getcwd(), image_path), 'wb') as f:\n",
    "            for chunk in image:\n",
    "                f.write(chunk)\n",
    "        return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:28<00:00, 14.76s/it, category=stock, index=0, news=udn]\n",
      "100%|██████████| 6/6 [01:25<00:00, 14.32s/it, category=sport, index=0, news=udn]\n",
      "100%|██████████| 6/6 [01:22<00:00, 13.75s/it, category=society, index=0, news=udn]\n",
      "100%|██████████| 10/10 [02:08<00:00, 12.80s/it, category=entertainment, index=0, news=udn]\n"
     ]
    }
   ],
   "source": [
    "class ScrapeApple(ScrapeLiberty):\n",
    "    \"\"\"Download apple daily news\n",
    "    \n",
    "    Functions:\n",
    "        scrape: scrape the news in categories\n",
    "    \"\"\"\n",
    "    def __init__(self, data=None):\n",
    "        ScrapeLiberty.__init__(self, data)\n",
    "        self.url_base = 'https://www.chinatimes.com'\n",
    "        self.categories = {'star': 'entertainment',\n",
    "                           'politic': 'politic',\n",
    "                           'sports': 'sport',\n",
    "                           'society': 'society'}\n",
    "        self.news = 'china'\n",
    "    \n",
    "    def scrape(self):\n",
    "        data = []\n",
    "        for category in self.categories.keys():\n",
    "            for index in range(1, 11):  \n",
    "                url = f'{self.url_base}/{category}/total?page={index}&chdtv'\n",
    "                response = get_page(url)\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "                news_list = soup.find('div', class_=\"container\") \\\n",
    "                                .find('ul', class_='vertical-list') \\\n",
    "                                .find_all('li')\n",
    "                \n",
    "                with tqdm(total=len(news_list)) as pbar:\n",
    "                    for news in news_list:\n",
    "                        try:\n",
    "                            datum = self.process_page(news, category)\n",
    "                        except: datum = None\n",
    "                        if datum:\n",
    "                            data.append(datum)\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix(index=index, category=self.categories[category], news=self.news)\n",
    "        \n",
    "        if self.data is None:\n",
    "            self.data = np.array(data)\n",
    "        else:\n",
    "            self.data = np.concatenate([np.array(data), self.data], dim=0)\n",
    "    \n",
    "    def process_page(self, news, category):\n",
    "        url = f'{self.url_base}{news.h3.a[\"href\"]}'\n",
    "        response = get_page(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        article = self.get_article(soup)\n",
    "\n",
    "        title = self.get_title(article)\n",
    "        if title in self.titles:\n",
    "            return None\n",
    "\n",
    "        date = self.get_date(article, category)\n",
    "        content = self.get_content(article, category)\n",
    "        image_path = self.get_image(news.img[\"src\"])\n",
    "\n",
    "        return [date, title, content, image_path, self.categories[category], url]\n",
    "    \n",
    "    def get_article(self, soup):\n",
    "        return soup.article\n",
    "    \n",
    "    def get_date(self, article, category):\n",
    "        pattern = '\\d{4}\\/\\d{2}\\/\\d{2}'\n",
    "        date = re.search(pattern, article.time.text).group()\n",
    "        date = datetime.strptime(date, '%Y/%m/%d').date()\n",
    "        return date\n",
    "\n",
    "    def get_content(self, article, category):\n",
    "        news_content_list = article.find('div', class_='article-body').find_all('p')\n",
    "        content = ''.join([content.text for content in news_content_list])\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# politic\n",
    "# https://tw.appledaily.com/politics/realtime\n",
    "# entertainment\n",
    "# https://tw.appledaily.com/entertainment/realtime\n",
    "# sport\n",
    "# https://tw.appledaily.com/sports/realtime\n",
    "# society\n",
    "# https://tw.appledaily.com/local/realtime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
